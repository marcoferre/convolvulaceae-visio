{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolvulaceae-visio.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PW-2wnxfSCcR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1yoVQA7IUZs"
      },
      "source": [
        "## Colab Configuration and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdOSt5oQjPQ8",
        "outputId": "bf384ee9-b6fb-4b27-adab-783aaad8ac8a"
      },
      "source": [
        "!pip install --upgrade pytube"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-11.0.1-py3-none-any.whl (56 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▉                          | 10 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 20 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 56 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-11.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUtoVTlbXnc"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import sklearn\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from scipy.stats import truncnorm\n",
        "import moviepy.editor as mpy\n",
        "import random\n",
        "import os\n",
        "import subprocess\n",
        "import pytube\n",
        "\n",
        "# filter out warnings regarding librosa.load for mp3s\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', '.*PySoundFile failed. Trying audioread instead*', )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW-2wnxfSCcR"
      },
      "source": [
        "## Download the Valence-Arousal inference model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmUMqfMvR_o2",
        "outputId": "418b52cb-089d-45e9-91b7-8ab97551bed7"
      },
      "source": [
        "#@markdown Setting the **vggish** to True allows to load and use the pre-trained \n",
        "#@markdown Vggish model to perform the Valence-Arousal regression of the audiotrack.\n",
        "#@markdown If **vggish** is False, then the MEL CNN Model is loaded.\n",
        "vggish = False #@param [\"False\", \"True\"] {type:\"raw\"} \n",
        "\n",
        "if vggish:\n",
        "  !wget --no-check-certificate -r \"https://drive.google.com/uc?export=download&id=1-DRbXRCEkB8qBZRdcE5miUW32szmiJy0\" -O \"VGGish_VA_model.h5\"\n",
        "else:\n",
        "  !wget --no-check-certificate -r \"https://drive.google.com/uc?export=download&id=1-sd4wy_OSB7ZoqSXkg-qOSLGwEiUqymA\" -O \"VA_model.h5\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2021-09-08 12:00:08--  https://drive.google.com/uc?export=download&id=1-sd4wy_OSB7ZoqSXkg-qOSLGwEiUqymA\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.111.138, 108.177.111.139, 108.177.111.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.111.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/s9ip8hka7qa0kgnqvtakr3pkm1sahb33/1631102400000/06980489334440521129/*/1-sd4wy_OSB7ZoqSXkg-qOSLGwEiUqymA?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-09-08 12:00:09--  https://doc-00-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/s9ip8hka7qa0kgnqvtakr3pkm1sahb33/1631102400000/06980489334440521129/*/1-sd4wy_OSB7ZoqSXkg-qOSLGwEiUqymA?e=download\n",
            "Resolving doc-00-ak-docs.googleusercontent.com (doc-00-ak-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-00-ak-docs.googleusercontent.com (doc-00-ak-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘VA_model.h5’\n",
            "\n",
            "VA_model.h5             [ <=>                ]   4.20M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-09-08 12:00:09 (103 MB/s) - ‘VA_model.h5’ saved [4401856]\n",
            "\n",
            "FINISHED --2021-09-08 12:00:09--\n",
            "Total wall clock time: 0.6s\n",
            "Downloaded: 1 files, 4.2M in 0.04s (103 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxi3-yMaRMqZ"
      },
      "source": [
        "## Load Model for the inference **of** Valence-Arousal "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KFRmsboQxVl"
      },
      "source": [
        "if vggish:\n",
        "  # Load the model VGGish CNN\n",
        "  model = tf.keras.models.load_model('/content/VGGish_VA_model.h5')\n",
        "else:\n",
        "  # Load the model MEL CNN \n",
        "  model = tf.keras.models.load_model('/content/VA_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asJz3F5MYDUs"
      },
      "source": [
        "## Load the audio file to be used for the video creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJAweYQ4oY53"
      },
      "source": [
        "Load the file from directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoKUKrYRN6TU"
      },
      "source": [
        "from google.colab import files \n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7fIBoAOocw1"
      },
      "source": [
        "Or load the file using a youtube link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YsJOU0PfjdFv",
        "outputId": "0a43b63f-21d1-46bf-981f-fadb5aafda0a"
      },
      "source": [
        "link = \"https://www.youtube.com/watch?v=ho9rZjlsyYY&ab_channel=MovieMongerHZ\"\n",
        "filename = \"/content/audiofile.mp4\"\n",
        "pytube.YouTube(link).streams.filter(only_audio=True).all()[1].download(filename=filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/audiofile.mp4'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKpiEpOF9pPN"
      },
      "source": [
        "#@markdown Duration of the video output in seconds.\n",
        "#@markdown It can be useful to generate shorter videos while you are tweaking the other visualizer parameters. \n",
        "#@markdown Once you find your preferred parameters, remove the duration argument to generate the video but for the full duration of the song (setting Duration = None).\n",
        "duration =  60#@param type number\n",
        "\n",
        "if vggish:\n",
        "  audio, sample_rate = librosa.load(filename, duration=duration,sr=16000)\n",
        "else:\n",
        "  audio, sample_rate = librosa.load(filename, duration=duration)\n",
        "\n",
        "import soundfile as sf\n",
        "sf.write('audiofile.wav', audio, sample_rate, 'PCM_24')\n",
        "\n",
        "audio_duration = np.floor(len(audio)/sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yETR_QmYTWD"
      },
      "source": [
        "## Analyse the audio file in segments and compute the Valence-Arousal values for each segment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeJYbC6FYN61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b475cc-d5f7-412c-cee3-97a76e90b5e6"
      },
      "source": [
        "# CNN input layer dimensions\n",
        "spec_h = 128\n",
        "spec_w = 216\n",
        "\n",
        "# normalizing the audio waveform\n",
        "audio = np.expand_dims(audio, axis=1)\n",
        "\n",
        "if vggish:\n",
        "  # Audio VGG is scaled at [-1.0,1.0] as per embedding input requirement\n",
        "  audio = sklearn.preprocessing.MinMaxScaler(feature_range=(-1.0, 1.0)).fit_transform(audio)\n",
        "else:\n",
        "  # Audio is normalized with standard scaler\n",
        "  audio = sklearn.preprocessing.StandardScaler().fit_transform(audio)\n",
        "\n",
        "audio = audio.flatten()\n",
        "\n",
        "waveforms = [] #VGGish INPUT \n",
        "specs = [] #CNN MEL INPUT\n",
        "\n",
        "# We split the audio in 5-second parts and compute the corresponding Mel Spectrogram in dB units. \n",
        "step = 5 \n",
        "for index, j in enumerate(tqdm(range(0,int(np.floor(len(audio)/sample_rate)), step))):\n",
        "  signal = audio[j * sample_rate : (j+step) * sample_rate]\n",
        "\n",
        "  if vggish:\n",
        "    # We pad each signal with zeros so to fit array with fixed dimensions\n",
        "    waveform = np.pad(signal,(0, step*sample_rate - signal.shape[0]))\n",
        "    waveforms.append(waveform)\n",
        "\n",
        "\n",
        "  # We extract the mel spectrogram (mel scale magnitude) in decibel (dB) units \n",
        "  spec = librosa.power_to_db(librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_mels=128, hop_length=int(2048/4)), ref=np.max)\n",
        "  \n",
        "  # Resize the mel spectrogram to match the CNN input dimensions\n",
        "  spec = np.expand_dims(spec, axis=-1) \n",
        "  specs.append(tf.image.resize(spec,[spec_h, spec_w]))\n",
        "  \n",
        "  sys.stderr.flush()\n",
        "\n",
        "# Store the spectrograms in a numpy array\n",
        "specs = np.asarray(specs, dtype=np.float16)\n",
        "waveforms = np.asarray(waveforms)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:00<00:00, 50.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW7a8x3Cfrff"
      },
      "source": [
        "## Predict the Valence-Arousal values for each segment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HQWieXUeqp4"
      },
      "source": [
        "if vggish:\n",
        "\n",
        "  # Load the pretrained vggish model.\n",
        "  import tensorflow_hub as hub\n",
        "  vggish_embeddings_model = hub.load('https://tfhub.dev/google/vggish/1')\n",
        "\n",
        "  # Compute the embeddings using the pretrained vggish model.\n",
        "  vggish_features = []\n",
        "  for waveform in waveforms:\n",
        "    vggish_features.append(np.expand_dims(vggish_embeddings_model(waveform), axis=-1))\n",
        "  vggish_features = np.array(vggish_features)\n",
        "\n",
        "  # predict the V-A values using the VGGish trained model\n",
        "  preds = model.predict(vggish_features)\n",
        "\n",
        "else:\n",
        "  preds = model.predict(specs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt1nCMnFr3QM"
      },
      "source": [
        "Write the Valence-Arousal values in a .json file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr5tAsJ8i4He",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b4a12c-3a47-4d98-fece-70cdf14958f9"
      },
      "source": [
        "!mkdir to_processing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘to_processing’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwkWb6YNp4-k"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('to_processing/predictions.json', 'w') as myfile:\n",
        "    json.dump(preds.tolist(), myfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk2KXynZqQA-"
      },
      "source": [
        "## Load the ImageNet classes Valence-Arousal mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIWcmcuxqPZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10b2ef7-79dd-44ea-efea-35f6e1f60fdf"
      },
      "source": [
        "!wget --no-check-certificate -r \"https://drive.google.com/uc?export=download&id=1CMnriEqxTi_HQXqbYkmbBOsOadIHthP3\" -O \"VA_ImageNet.csv.zip\"\n",
        "!unzip \"/content/VA_ImageNet.csv.zip\"\n",
        "brm_df = pd.read_csv('/content/imagenet-classes-v-a_scaled.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2021-09-08 12:00:20--  https://drive.google.com/uc?export=download&id=1CMnriEqxTi_HQXqbYkmbBOsOadIHthP3\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.201.101, 74.125.201.138, 74.125.201.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.201.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-9c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6jlqh22qjvbv44teml4qcna6v7h6rgvi/1631102400000/17562262531043783088/*/1CMnriEqxTi_HQXqbYkmbBOsOadIHthP3?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-09-08 12:00:21--  https://doc-0c-9c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6jlqh22qjvbv44teml4qcna6v7h6rgvi/1631102400000/17562262531043783088/*/1CMnriEqxTi_HQXqbYkmbBOsOadIHthP3?e=download\n",
            "Resolving doc-0c-9c-docs.googleusercontent.com (doc-0c-9c-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-0c-9c-docs.googleusercontent.com (doc-0c-9c-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11925 (12K) [application/x-zip-compressed]\n",
            "Saving to: ‘VA_ImageNet.csv.zip’\n",
            "\n",
            "VA_ImageNet.csv.zip 100%[===================>]  11.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-08 12:00:21 (46.1 MB/s) - ‘VA_ImageNet.csv.zip’ saved [11925/11925]\n",
            "\n",
            "FINISHED --2021-09-08 12:00:21--\n",
            "Total wall clock time: 0.7s\n",
            "Downloaded: 1 files, 12K in 0s (46.1 MB/s)\n",
            "Archive:  /content/VA_ImageNet.csv.zip\n",
            "replace imagenet-classes-v-a_scaled.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: imagenet-classes-v-a_scaled.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubIuDwa9y0mo"
      },
      "source": [
        "Search for the classes that correspond to the Valence Arousal of each segment spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tARX7a9q-lR"
      },
      "source": [
        "brm_df_drop = brm_df.drop(columns=['Id','Class', 'WordBRM'])\n",
        "\n",
        "brm_arr = brm_df_drop.to_numpy()\n",
        "\n",
        "\n",
        "def find_classes(valence, arousal, length, np_dataset, df_dataset):\n",
        "    tree = sp.spatial.KDTree(np_dataset)\n",
        "    distance, brm_indices = tree.query([valence, arousal], length)\n",
        "    return df_dataset.values[brm_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmZ30Nh90uiN"
      },
      "source": [
        "## Configure the Visualizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA1_qsNGz3hL"
      },
      "source": [
        "#@markdown Select the **resolution** of the images to be generated\n",
        "resolution = 512 #@param [128, 256, 512]\n",
        "\n",
        "#@markdown The **frame length** controls the number of audio frames per video frame in the output.\n",
        "#@markdown If you want a higher frame rate for visualizing very rapid music, lower the frame_length   \n",
        "#@markdown (Range: Multiples of 2^6).\n",
        "frame_length = 512 #@param {type:\"slider\", min:64, max:1024, step:64}\n",
        "\n",
        "\n",
        "batch_size = 15 # Set batch size\n",
        "# BigGAN generates the images in batches of size [batch_size]. \n",
        "# The only reason to reduce batch size from the default of 30 is if you run out of memory on a GPU. \n",
        "# Reducing the batch size will slightly increase overall runtime.\n",
        "\n",
        "\n",
        "if duration:\n",
        "    seconds = duration\n",
        "    frame_lim = int(np.floor(seconds*22050/frame_length/batch_size))\n",
        "else:\n",
        "    frame_lim = int(np.floor(len(audio)/sample_rate*22050/frame_length/batch_size))\n",
        "\n",
        "\n",
        "#@markdown The **truncation** trick provides a trade-off between image quality or fidelity and image variety.\n",
        "#@markdown A more narrow sampling range results in better quality, whereas a larger sampling range results in more variety in sampled images\n",
        "#@markdown (scalar truncation value in [0.0, 1.0]).\n",
        "\n",
        "#@markdown The truncation trick involves using a different distribution for the generator’s \n",
        "#@markdown latent space during training than during inference or image synthesis.\n",
        "#@markdown A Gaussian distribution is used during training, and a truncated Gaussian is \n",
        "#@markdown used during inference. This is referred to as the “truncation trick.”\n",
        "#@markdown We call this the Truncation Trick: truncating a z vector by resampling the values\n",
        "#@markdown with magnitude above a chosen threshold leads to improvement in individual sample\n",
        "#@markdown quality at the cost of reduction in overall sample variety.\n",
        "\n",
        "#@markdown The truncation controls the variability of images that BigGAN generates by\n",
        "#@markdown limiting the max values in the noise vector. \n",
        "#@markdown Truncations closer to 1 yield more variable images, and truncations closer to \n",
        "#@markdown 0 yield simpler images with more recognizable, normal-looking objects.\n",
        "truncation = 1  #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown The **pitch sensitivity** controls how rapidly the class vector (thematic content of the video)\n",
        "#@markdown will react to changes in pitch. The higher the number, the higher the sensitivity.\n",
        "#@markdown Recommended Range: [200, 295]\n",
        "pitch_sensitivity = 220 #@param {type:\"slider\", min:1, max:299, step:1}\n",
        "\n",
        "pitch_sensitivity = (300 - pitch_sensitivity) * 512 / frame_length\n",
        "\n",
        "#@markdown The **tempo sensitivity** controls how rapidly the noise vector\n",
        "#@markdown will react to changes in audio. The higher the number, the higher the sensitivity.\n",
        "#@markdown (Range: [0.05, 0.8]).\n",
        "tempo_sensitivity = 0.25 # @param {type:\"slider\", min:0.05, max:0.8, step:0.05}\n",
        "\n",
        "tempo_sensitivity = tempo_sensitivity * frame_length / 512\n",
        "\n",
        "#@markdown The **depth** specifies the max value of the class vector units. Numbers closer to 1 \n",
        "#@markdown seem to yield more thematically rich content. Numbers closer to 0 seem to yield\n",
        "#@markdown more 'deep' structures like human and dog faces. However, this depends heavily\n",
        "#@markdown on the specific classes you are using (Range: [0.01, 1.0])\n",
        "depth = 1 #@param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
        "\n",
        "#@markdown Set **number of classes**, Default: Twelve random indices between 0-999.\n",
        "#@markdown Default is twelve, corresponding to the twelve musical pitches (A, A#, B, etc.)). \n",
        "num_classes = 12 # @param {type:\"slider\", min:1, max:12, step:1}\n",
        "\n",
        "#@markdown The **jitter** prevents the same exact noise vectors from cycling repetitively during repetitive music\n",
        "#@markdown so that the video output is more interesting. If you do want to cycle repetitively, set jitter to 0.\n",
        "#@markdown (Range: [0.0, 1.0])\n",
        "jitter = 0.8 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1} \n",
        "\n",
        "use_previous_vectors = 0 # Set use_previous_classes\n",
        "use_previous_classes = 0 # Set use_previous_vectors\n",
        "    \n",
        "outname = 'convolvulaceae-visio.mp4' # Set output name\n",
        "\n",
        "#@markdown After the class vectors have been generated, they are smoothed by interpolating linearly between\n",
        "#@markdown the means of class vectors in bins of size [smooth_factor]. This is performed because small local \n",
        "#@markdown fluctuations in pitch can cause the video frames to fluctuate back and forth. \n",
        "#@markdown If you want to visualize very fast music with rapid changes in pitch, you can lower the smooth factor. \n",
        "#@markdown You may also want to lower the frame_length in that case. However, for most songs, it is difficult to avoid \n",
        "#@markdown rapid fluctuations with smooth factors less than 10.\n",
        "#@markdown Set smooth factor in Range [1 – 30]\n",
        "smooth_factor = 1 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "if smooth_factor > 1:\n",
        "    smooth_factor = int(smooth_factor * 512 / frame_length)\n",
        "else:\n",
        "    smooth_factor = smooth_factor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGNZ8azM6KsN"
      },
      "source": [
        "## Adding the Visualizer functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq8aiH4e0-rb"
      },
      "source": [
        "# Create a truncated noise sample\n",
        "def truncated_noise_sample(batch_size=1, dim_z=128, truncation=1., seed=None):\n",
        "    \"\"\" Create a truncated noise vector.\n",
        "        Params:\n",
        "            batch_size: batch size.\n",
        "            dim_z: dimension of z\n",
        "            truncation: truncation value to use\n",
        "            seed: seed for the random generator\n",
        "        Output:\n",
        "            array of shape (batch_size, dim_z)\n",
        "    \"\"\"\n",
        "    state = None if seed is None else np.random.RandomState(seed)\n",
        "    values = truncnorm.rvs(-2, 2, size=(batch_size, dim_z), random_state=state).astype(np.float32)\n",
        "    return (truncation * values).astype(np.float32)\n",
        "\n",
        "\n",
        "def convert_to_images(obj):\n",
        "    \"\"\" Convert an output tensor from BigGAN in a list of images.\n",
        "        Params:\n",
        "            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n",
        "        Output:\n",
        "            list of Pillow Images of size (height, width)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import PIL\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install Pillow to use images: pip install Pillow\")\n",
        "\n",
        "    if not isinstance(obj, np.ndarray):\n",
        "        obj = obj.detach.numpy()\n",
        "\n",
        "    img = []\n",
        "    for i, out in enumerate(obj):\n",
        "        out_array = np.asarray(np.uint8(out), dtype=np.uint8)\n",
        "        img.append(PIL.Image.fromarray(out_array))\n",
        "    return img\n",
        "\n",
        "\n",
        "def predict(sess, noise, label, truncation=1.):\n",
        "  noise = np.asarray(noise)\n",
        "  label = np.asarray(label)\n",
        "  num = noise.shape[0]\n",
        "  feed_dict = {input_z: noise, input_y: label, input_trunc: truncation}\n",
        "  ims = sess.run(output, feed_dict=feed_dict)\n",
        "  ims = np.clip(((ims + 1) / 2.0) * 256, 0, 255)\n",
        "  ims = np.uint8(ims)\n",
        "  return ims\n",
        "\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  a = np.asarray(a, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(a).save(data, format)\n",
        "  im_data = data.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print(('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format))\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def imgrid(imarray, cols=5, pad=1):\n",
        "  if imarray.dtype != np.uint8:\n",
        "    raise ValueError('imgrid input imarray must be uint8')\n",
        "  pad = int(pad)\n",
        "  assert pad >= 0\n",
        "  cols = int(cols)\n",
        "  assert cols >= 1\n",
        "  N, H, W, C = imarray.shape\n",
        "  rows = N // cols + int(N % cols != 0)\n",
        "  batch_pad = rows * cols - N\n",
        "  assert batch_pad >= 0\n",
        "  post_pad = [batch_pad, pad, pad, 0]\n",
        "  pad_arg = [[0, p] for p in post_pad]\n",
        "  imarray = np.pad(imarray, pad_arg, 'constant', constant_values=255)\n",
        "  H += pad\n",
        "  W += pad\n",
        "  grid = (imarray\n",
        "          .reshape(rows, cols, H, W, C)\n",
        "          .transpose(0, 2, 1, 3, 4)\n",
        "          .reshape(rows*H, cols*W, C))\n",
        "  if pad:\n",
        "    grid = grid[:-pad, :-pad]\n",
        "  return grid\n",
        "\n",
        "# Get new jitters\n",
        "def new_jitters(jitter):\n",
        "  jitters = np.zeros(128)\n",
        "  for j in range(128):\n",
        "      if random.uniform(0, 1) < 0.5:\n",
        "          jitters[j] = 1\n",
        "      else:\n",
        "          jitters[j] = 1-jitter\n",
        "  return jitters\n",
        "\n",
        "\n",
        "# Get new update directions\n",
        "def new_update_dir(nv2, update_dir):\n",
        "  for ni, n in enumerate(nv2):\n",
        "      if n >= 2*truncation - tempo_sensitivity:\n",
        "          update_dir[ni] = -1  \n",
        "                      \n",
        "      elif n < -2*truncation + tempo_sensitivity:\n",
        "          update_dir[ni] = 1   \n",
        "  return update_dir\n",
        "\n",
        "\n",
        "# Smooth class vectors\n",
        "def smooth(class_vectors, smooth_factor):\n",
        "  if smooth_factor == 1:\n",
        "      return np.array(class_vectors)\n",
        "  \n",
        "  class_vectors_terp = []\n",
        "  for c in range(int(np.floor(len(class_vectors)/smooth_factor) - 1)):\n",
        "      ci = c*smooth_factor\n",
        "      cva = np.mean(class_vectors[int(ci):int(ci)+smooth_factor], axis=0)\n",
        "      cvb = np.mean(class_vectors[int(ci)+smooth_factor:int(ci)+smooth_factor*2], axis=0)\n",
        "                  \n",
        "      for j in range(smooth_factor):                                 \n",
        "          cvc = cva*(1 - j/(smooth_factor - 1)) + cvb*(j/(smooth_factor - 1))\n",
        "          class_vectors_terp.append(cvc)\n",
        "          \n",
        "  return np.array(class_vectors_terp)\n",
        "\n",
        "\n",
        "# Normalize class vector between 0-1\n",
        "def normalize_cv(cv2):\n",
        "  min_class_val = min(i for i in cv2 if i != 0)\n",
        "  for ci, c in enumerate(cv2):\n",
        "      if c == 0:\n",
        "          cv2[ci] = min_class_val\n",
        "  cv2 = (cv2-min_class_val)/np.ptp(cv2)\n",
        "  \n",
        "  return cv2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAggBjDv6aZO"
      },
      "source": [
        "## Compute the audio mel spectrogram and chromagram. \n",
        " - The mel spectrogram yields the mean power and its gradient is used to update the noise vectors.\n",
        " - The chromagram yields the power corresponding to each of the 12 pitches and its used to initialize the class vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj7zLN8U0_Ga"
      },
      "source": [
        "# Create spectrogram\n",
        "spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128, fmax=8000, hop_length=frame_length)\n",
        "\n",
        "# Get mean power at each time point\n",
        "specm = np.mean(spec, axis=0)\n",
        "\n",
        "# Compute power gradient across time points\n",
        "gradm = np.gradient(specm)\n",
        "\n",
        "# Set max to 1\n",
        "gradm = gradm/np.max(gradm)\n",
        "\n",
        "# Set negative gradient time points to zero\n",
        "gradm = gradm.clip(min=0)\n",
        "    \n",
        "# Normalize mean power between 0-1\n",
        "specm = (specm-np.min(specm))/np.ptp(specm)\n",
        "\n",
        "# Create chromagram of pitches X time points\n",
        "chroma = librosa.feature.chroma_cqt(y=audio, sr=sample_rate, hop_length=frame_length)\n",
        "\n",
        "# Sort pitches by overall power\n",
        "chromasort = np.argsort(np.mean(chroma, axis=1))[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-5UBxpi6UAr",
        "outputId": "10d2b022-a1ed-4489-a651-8be83334c3c0"
      },
      "source": [
        "classes = np.zeros((len(gradm),num_classes))\n",
        "\n",
        "prediction_index = -1 \n",
        "for frame_index, grad in enumerate(tqdm(range(len(gradm)))): \n",
        "  if frame_index % np.ceil((5*sample_rate)/frame_length) == 0: \n",
        "    prediction_index = prediction_index + 1 \n",
        "  classes[frame_index][:] = find_classes(preds[prediction_index, 0], preds[prediction_index, 1], num_classes, brm_arr, brm_df)[:,0]\n",
        "    \n",
        "  sys.stderr.flush()\n",
        "\n",
        "# Initialize first class vector\n",
        "cv1 = np.zeros(1000,dtype=np.float32)\n",
        "for pi, p in enumerate(chromasort[:num_classes]):\n",
        "  if num_classes < 12:\n",
        "    cv1[int(classes[0][pi])] = chroma[p][np.min([np.where(chrow > 0)[0][0] for chrow in chroma])]\n",
        "  else:\n",
        "    cv1[int(classes[0][p])] = chroma[p][np.min([np.where(chrow > 0)[0][0] for chrow in chroma])]\n",
        "\n",
        "# Initialize first noise vector\n",
        "nv1 = truncated_noise_sample(truncation=truncation)[0]\n",
        "\n",
        "# Initialize list of class and noise vectors\n",
        "class_vectors = [cv1]\n",
        "noise_vectors = [nv1]\n",
        "\n",
        "# Initialize previous vectors (will be used to track the previous frame)\n",
        "cvlast = cv1\n",
        "nvlast = nv1\n",
        "\n",
        "# Initialize the direction of noise vector unit updates\n",
        "update_dir = np.zeros(128)\n",
        "for ni, n in enumerate(nv1):\n",
        "  if n < 0:\n",
        "    update_dir[ni] = 1\n",
        "  else:\n",
        "    update_dir[ni] = -1\n",
        "\n",
        "# initialize noise unit update\n",
        "update_last = np.zeros(128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2584/2584 [00:32<00:00, 80.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpjZRsEFOav9"
      },
      "source": [
        "Generating input vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM6aC6KjKzXJ",
        "outputId": "dddb4b49-387c-4702-9c79-34a7ffe25fdc"
      },
      "source": [
        "for i in tqdm(range(len(gradm))):   \n",
        "    \n",
        "  # Print progress\n",
        "  pass\n",
        "  \n",
        "  # Update jitter vector every 100 frames by setting ~half of noise vector units to lower sensitivity\n",
        "  if i % 200 == 0:\n",
        "      jitters = new_jitters(jitter)\n",
        "  \n",
        "  # Get last noise vector\n",
        "  nv1 = nvlast\n",
        "  \n",
        "  # Set noise vector update based on direction, sensitivity, jitter, and combination of\n",
        "  # overall power and gradient of power\n",
        "  update = np.array([tempo_sensitivity for k in range(128)]) * (gradm[i]+specm[i]) * update_dir * jitters \n",
        "  \n",
        "  # Smooth the update with the previous update (to avoid overly sharp frame transitions)\n",
        "  update = (update+update_last * 3) / 4\n",
        "  \n",
        "  # Set last update\n",
        "  update_last = update\n",
        "      \n",
        "  # Update noise vector\n",
        "  nv2 = nv1+update\n",
        "  \n",
        "  # Append to noise vectors\n",
        "  noise_vectors.append(nv2)\n",
        "  \n",
        "  # Set last noise vector\n",
        "  nvlast = nv2\n",
        "                 \n",
        "  # Update the direction of noise units\n",
        "  update_dir = new_update_dir(nv2, update_dir)\n",
        "  \n",
        "  # Get last class vector\n",
        "  cv1 = cvlast\n",
        "  \n",
        "  # Generate new class vector\n",
        "  cv2 = np.zeros(1000)\n",
        "  for j in range(num_classes):\n",
        "    cv2[int(classes[i][j])] = (cvlast[int(classes[i][j])] + ((chroma[chromasort[j]][i])/pitch_sensitivity))/(1 + (1/pitch_sensitivity))\n",
        "  \n",
        "  # If more than 6 classes, normalize new class vector between 0 and 1, else simply set max class val to 1\n",
        "  if num_classes > 6:\n",
        "    cv2 = normalize_cv(cv2)\n",
        "  else:\n",
        "    cv2 = cv2/np.max(cv2)\n",
        "  \n",
        "  # Adjust depth\n",
        "  cv2 = cv2*depth\n",
        "  \n",
        "  # This prevents rare bugs where all classes are the same value\n",
        "  if np.std(cv2[np.where(cv2 != 0)]) < 0.0000001:\n",
        "    cv2[int(classes[i][0])] = cv2[int(classes[i][0])] + 0.01  \n",
        "  \n",
        "  # Append new class vector\n",
        "  class_vectors.append(cv2)\n",
        "\n",
        "  # Set last class vector\n",
        "  cvlast = cv2\n",
        "\n",
        "\n",
        "# Interpolate between class vectors of bin size [smooth_factor] to smooth frames\n",
        "class_vectors = smooth(class_vectors, smooth_factor)\n",
        "\n",
        "\n",
        "# Check whether to use vectors from last run\n",
        "if use_previous_vectors == 1:\n",
        "    # Load vectors from previous run\n",
        "    class_vectors = np.load('class_vectors.npy')\n",
        "    noise_vectors = np.load('noise_vectors.npy')\n",
        "else:\n",
        "    # Save record of vectors for current video\n",
        "    np.save('class_vectors.npy', class_vectors)\n",
        "    np.save('noise_vectors.npy', noise_vectors)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2584/2584 [00:03<00:00, 834.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dxoGRwEkRtr"
      },
      "source": [
        "Write the classes with their Valence and Arousal into a .json file to be used for Processing visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFf0XtrPZpO6"
      },
      "source": [
        "classes_unique = []\n",
        "class_va = dict()\n",
        "\n",
        "for frame_index, class_vector in enumerate(classes):\n",
        "    if frame_index % np.ceil((5*sample_rate)/frame_length) == 0:\n",
        "        for cl in class_vector:\n",
        "            class_va[int(cl)] = brm_df.loc[brm_df['Id'] == int(cl), \"Class\":\"A.Mean.Sum\"].values[0].tolist()\n",
        "        classes_unique.append(class_va)\n",
        "    class_va = dict()\n",
        "\n",
        "import json\n",
        "\n",
        "with open('to_processing/classes_va.json', 'w') as myfile:\n",
        "    json.dump(classes_unique, myfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEfNdneJPGNy"
      },
      "source": [
        "Load BigGAN generator module from TF Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYTlqHm9R5qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d21d760-eb73-470c-cb0e-a31a682f2775"
      },
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "tf1.disable_v2_behavior()\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5-Reys7PAwQ",
        "outputId": "be65feaf-7d65-46ba-d40d-34efae41395a"
      },
      "source": [
        "module_path = 'https://tfhub.dev/deepmind/biggan-deep-' + str(resolution) + '/1'\n",
        "print('Loading BigGAN module from:', module_path)\n",
        "\n",
        "tf1.reset_default_graph()\n",
        "module = hub.Module(module_path)\n",
        "\n",
        "inputs = {k: tf1.placeholder(v.dtype, v.get_shape().as_list(), k)\n",
        "          for k, v in module.get_input_info_dict().items()}\n",
        "output = module(inputs)\n",
        "\n",
        "input_z = inputs['z']\n",
        "input_y = inputs['y']\n",
        "input_trunc = inputs['truncation']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BigGAN module from: https://tfhub.dev/deepmind/biggan-deep-512/1\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJx7f8q5O8aC"
      },
      "source": [
        "Create a TensorFlow session and initialize variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaEJcz2dO3bc"
      },
      "source": [
        "initializer = tf1.global_variables_initializer()\n",
        "sess = tf1.Session()\n",
        "sess.run(initializer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQeF78jWPwln"
      },
      "source": [
        "Generate frames in batches of batch_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAjfKqmAPt6c",
        "outputId": "454ee5f4-038d-406e-abb0-9e385bd18391"
      },
      "source": [
        "frames = []\n",
        "\n",
        "for i in tqdm(range(frame_lim)):\n",
        "    \n",
        "  # Print progress\n",
        "  pass\n",
        "  \n",
        "  if (i + 1) * batch_size > class_vectors.shape[0]:\n",
        "      tf1.keras.backend.clear_session()\n",
        "      break\n",
        "  \n",
        "  # Get batch\n",
        "  noise_vector = np.asarray(noise_vectors[i*batch_size:(i + 1)*batch_size], dtype=np.float32)\n",
        "  class_vector = np.asarray(class_vectors[i*batch_size:(i + 1)*batch_size], dtype=np.float32)\n",
        "  # Generate images\n",
        "  predictions = predict(sess, noise_vector, class_vector, truncation=truncation)\n",
        "\n",
        "  # Convert to image array and add to frames\n",
        "  output_images = convert_to_images(predictions)\n",
        "  for out in output_images:\n",
        "      im = np.array(out)\n",
        "      frames.append(im)\n",
        "\n",
        "# Save video\n",
        "aud = mpy.AudioFileClip('/content/audiofile.wav', fps=44100)\n",
        "\n",
        "\n",
        "clip = mpy.ImageSequenceClip(frames, fps=22050/frame_length)\n",
        "clip = clip.set_audio(aud)\n",
        "clip.write_videofile(outname, audio_codec='aac')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 172/172 [06:02<00:00,  2.11s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/moviepy/audio/io/readers.py:116: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  result = np.fromstring(s, dtype=dt)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] >>>> Building video convolvulaceae-visio.mp4\n",
            "[MoviePy] Writing audio in convolvulaceae-visioTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1324/1324 [00:02<00:00, 562.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] Writing video convolvulaceae-visio.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 2580/2580 [01:21<00:00, 31.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] >>>> Video ready: convolvulaceae-visio.mp4 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCWpZcuarJut"
      },
      "source": [
        "Download Files to be used for Processing visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "7jEttcznmi85",
        "outputId": "aa3bca01-a272-4454-a529-7770ae0a14c3"
      },
      "source": [
        "!zip /content/convolvulaceae-visio.zip /content/convolvulaceae-visio.mp4\n",
        "!zip -r /content/to_processing.zip /content/to_processing\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/convolvulaceae-visio.zip\")\n",
        "files.download(\"/content/to_processing.zip\")\n",
        "files.download(\"/content/audiofile.wav\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/convolvulaceae-visio.mp4 (deflated 0%)\n",
            "updating: content/to_processing/ (stored 0%)\n",
            "updating: content/to_processing/predictions.json (deflated 52%)\n",
            "updating: content/to_processing/classes_va.json (deflated 88%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cc1e2a67-9c2b-47a5-b234-3804cd842da0\", \"convolvulaceae-visio.zip\", 27949166)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e2bfcb8b-ec75-419e-8b14-2042089bb241\", \"to_processing.zip\", 2095)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6959a6d5-7b46-4494-97d7-3d1748dd8c7f\", \"audiofile.wav\", 3969044)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}